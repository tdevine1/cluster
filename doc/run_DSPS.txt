# concatenate cluster files
cat *.txt > palfa_001-clusters.csv
sed -i '/survey.*/d' ./palfa_001-clusters.csv 
sed -i -e '1isurvey,mjd,pointing,beam,clusterrank,merged,clustersize,centerDMspacing,center,DM,center,time,center,SNR,center,sampling,center,downfact,min,DM,max,DM,min,time,max,time\' palfa_001-clusters.csv 

# load files to cluster
hadoop fs -put /home/hduser/data/palfa/palfa_001.csv /data/palfa/palfa_001.csv
hadoop fs -put /home/hduser/data/palfa/palfa_001-clusters.csv /data/palfa/palfa_001-clusters.csv

# run a spark job on the loaded data
spark-submit --class DSPS --master yarn --deploy-mode cluster --driver-memory 7g --num-executors 29 --executor-memory 3gm --executor-cores 2 --conf spark.yarn.executor.memoryOverhead=1536 --queue default /home/hduser/workspace-scala/DSPS/target/DSPS-0.0.1-SNAPSHOT.jar /data/palfa/palfa_001-clusters.csv /data/palfa/palfa_001.csv /user/hduser/results/palfa/palfa_001

