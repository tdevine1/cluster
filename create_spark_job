# spin up the spark cluster
/usr/hdp/2.4.2.0-258/spark/sbin/start-all.sh

# make directory structure for scala code
mkdir -p [projectname]/src/main/scala

# move into [projectname]
cd [projectname]

# make sbt file
nano [projectname].sbt

# copy info into file
name := "[projectname]"
 
version := "1.0"

scalaVersion := "2.11.8"

libraryDependencies += "org.apache.spark" %% "spark-core" % "1.6.1"
# provide additional dependencies

# package the code to make a runnable job
sbt package

# submit the job
spark-submit --class [main_class_name] --master yarn --deploy-mode cluster --driver-memory 4g --executor-memory 2g --executor-cores 3 --queue default [full_path_to_projectname]_2.11-1.0.jar 
